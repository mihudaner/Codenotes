# Xduts

### [超详细2024版Latex安装Texlive+Texstudio（含环境配置）](https://blog.csdn.net/Luan__Yu/article/details/143562703)

### [讨论区](https://github.com/note286/xduts/discussions)

### [本文简单介绍如何在Windows中使用**Texlive+vscode LaTeX Workshop插件**编写xduts模板的论文](https://zhuanlan.zhihu.com/p/696132299)

### [【Texlive + XDUTS + Texstudio】](https://blog.csdn.net/i013140225/article/details/128373751)

```
texdoc xduts
```



无法跳转是路径有中文，编译一堆报错是重写安装的，

`Undefined control sequence` 表示 LaTeX 找不到某个命令的定义 一般是因为没有\usepackage{todonotes}



```
在这里添加另一个批注\todo[inline]{这是一个内联批注。}
```

<img src="E:\codenotes\方向\img\image-20241231133922043.png" alt="image-20241231133922043" style="zoom:50%;" />



### depthanying

 [深度转点云点云着色](https://blog.csdn.net/jacke121/article/details/144171915)

[深度转点云](https://itadn.com/personalcenter/unlockrecord)

### [检索系统](https://libthesis.xidian.edu.cn/searchPage?papertype=paper&searchKeyWord)

## 对应

```
第一个点：

​	问题：点云稀疏问题和图像深度相关任务的非线性距离误差分布

​	解决办法是用图像点云超分辨和组合滤波来滤除的

第二个点：

​	问题：语义的边缘溢出，以及远处小目标稀疏

​	解决办法是用互投影和像素提升虚拟点


```

## 研究背景



### 强光,黑夜

https://www.seyond.cn/%e5%bc%ba%e5%85%89%e4%b8%8b%e8%87%b4%e7%9b%b2%ef%bc%9f%e5%bc%80%e5%90%af%e6%bf%80%e5%85%89%e9%9b%b7%e8%be%be%e5%a4%96%e6%8c%82%ef%bc%8c%e6%97%a0%e6%83%a7%e5%b9%b2%e6%89%b0/

https://www.seyond.cn/%e6%bf%80%e5%85%89%e9%9b%b7%e8%be%be%e7%9a%84%e8%b6%85%e6%b8%85%e8%a7%86%e7%95%8c%ef%bc%9a%e7%bb%86%e8%8a%82%e5%b0%bd%e4%ba%ab-%e5%ae%89%e5%85%a8%e5%8a%a0%e5%80%8d/

## 研究现状

| [单目现状](https://mp.weixin.qq.com/s/-KmeYIJkt4EVhhGnwh4Ubw) |                                                              |
| :----------------------------------------------------------- | ------------------------------------------------------------ |
| 双目现状                                                     | fully space fusion,自动驾驶中的三维目标检测算法研究综述      |
| 激光雷达现状                                                 | fully space fusion, virpnet                                  |
|                                                              |                                                              |
|                                                              |                                                              |
|                                                              |                                                              |
|                                                              |                                                              |
|                                                              |                                                              |
| 长尾效应                                                     | Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud  |
| 边界模糊效应                                                 | FusionPainting https://blog.csdn.net/m0_63604019/article/details/126941874 |
| SparseBEV                                                    | ×   写的不好，都是bev                                        |





### 点云聚类

[激光点云的物体聚类](https://mp.weixin.qq.com/s/UBmu5FnYhfxuPH2gV7MRBg)

<img src="E:\codenotes\方向\img\image-20250218204121041.png" alt="image-20250218204121041" style="zoom:50%;" />

<img src="E:\codenotes\方向\img\image-20250218204054993.png" alt="image-20250218204054993" style="zoom:50%;" />

<img src="E:\codenotes\方向\img\image-20250218204140111.png" alt="image-20250218204140111" style="zoom:50%;" />

<img src="E:\codenotes\方向\img\image-20250218204358575.png" alt="image-20250218204358575" style="zoom:50%;" />

<img src=".\img\image-20250218204425708.png" alt="image-20250218204425708" style="zoom:50%;" />

![image-20250219162537931](.\img\image-20250219162537931.png)

![image-20250220192219641](E:\codenotes\方向\img\image-20250220192219641.png)











# 深度



1. **"DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image"** (CVPR 2019): 这篇文章可能使用稀疏LiDAR数据和单目图像来生成密集深度图，但可能更侧重于利用表面法线而不是直接矫正误差。
2. **"Sparse and Dense Data with CNNs: Depth Completion and Semantic Segmentation"** (GCPR 2018): 这篇可能涉及如何结合稀疏和密集数据，但主要是在深度补全任务中，可能相关。
3. **"LiDAR-enhanced Structure-from-Motion"** (可能存在的论文)：这类方法可能用LiDAR来提升SfM的精度，从而改善深度估计。
4. **"Guided Depth Enhancement via a Fast Marching Method"** (TIP 2013): 使用引导滤波或其他传统方法进行深度图增强，可能适用于有稀疏真实点的情况。
5. **"Depth Map Super-Resolution Using Sparse Depth Information"** (CVPR 2018): 这篇可能更关注超分辨率，但方法可能适用于矫正误差。
6. **"AdaBins: Depth Estimation using Adaptive Bins"** (CVPR 2021): 虽然主要解决深度估计的分布问题，但可能对误差矫正有启发。
7. **"Learning Depth from Monocular Videos using Direct Methods"** (CVPR 2018): 结合其他传感器数据进行深度估计的矫正。

另外，可能有一些近期的工作在伪激光雷达（Pseudo-LiDAR）领域，比如：

- **"Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving"** (可能即用户提到的Pseudo++)，但还有其他相关工作吗？

比如：

- **"LiDAR-aid Pseudo-LiDAR for 3D Object Detection"** (可能某个论文中的方法)：利用LiDAR数据辅助生成更准确的伪激光雷达点云。

此外，可能有一些关于多模态融合的方法，比如：

- **"PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation"** (CVPR 2018)：融合点云和图像数据进行检测，可能对融合方法有参考。

比如，在深度补全领域，KITTI深度补全竞赛中的很多工作可能相关，例如：

- **"Sparse and noisy LiDAR completion with RGB guidance and uncertainty"** (MF 2019): 使用RGB和稀疏LiDAR数据进行深度补全，可能涉及矫正。
- **"Deep Depth Completion of a Single RGB-D Image"** (CVPR 2018): 虽然针对RGB-D，但方法可能适用。

另外，**"Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera"** (ICRA 2019) 这篇文章可能使用自监督方法，结合LiDAR稀疏数据和相机图像。

在矫正非线性误差方面，可能有一些工作将非线性映射建模为神经网络。例如，训练一个网络，输入是估计的深度图，输出是矫正后的深度图，其中稀疏的真实点作为监督信号。

此外，可能有一些基于传统优化的方法，如使用高斯过程回归，在稀疏点处建立误差模型，并预测整个深度图的误差。

例如，**"Non-parametric Depth Correction for Pseudo-LiDAR"**（假设的论文名称）可能使用非参数方法进行矫正。



- **"Depth Correction from RGB-D Sensors via Topology Alignment"** (TPAMI 2015)：可能使用拓扑对齐来矫正深度误差。
- **"Learning to Refine Depth for Robust Visual Perception"** (WACV 2020)：学习如何优化深度图以提高感知任务的鲁棒性。

例如，在CVPR 2021中，**"3D Spatial Recognition without Spatially Labeled 3D"** 提到了利用深度信息，但不确定是否相关。

另外，**"Unsupervised Depth Completion from Visual Inertial Odometry"** (ICRA 2020) 可能使用其他传感器数据来辅助深度补全。

再比如，**"Sparse Auxiliary Networks for Unified Monocular Depth Prediction and Completion"** (CVPR 2021) 这篇文章可能结合深度预测和补全，使用稀疏的深度输入。

例如，**"Sparse-to-Dense Depth Completion Using a Multi-Scale Network with Attention"** (Sensors 2021) 使用多尺度和注意力机制进行深度补全。





------

### **1. 深度补全（Depth Completion）与稀疏监督**

这类方法利用稀疏LiDAR点云作为监督信号，结合RGB图像生成密集深度图，可迁移至深度误差矫正任务。

- 推荐论文：
  - **《DeepLiDAR: Deep Surface Normal Guided Depth Prediction for Outdoor Scene from Sparse LiDAR Data and Single Color Image》 (CVPR 2019)**
    结合稀疏LiDAR数据和单目图像，通过表面法线约束生成密集深度图，适合从稀疏到密集的深度生成与矫正。
  - **《Sparse Auxiliary Networks for Unified Monocular Depth Prediction and Completion》 (CVPR 2021)**
    提出统一框架，同时处理深度预测与补全，利用稀疏点云作为辅助输入，适合联合优化密集深度误差。

------

### **2. 基于非刚性配准的矫正**

通过匹配稀疏真实点云与密集伪点云，估计非线性变形场（如TPS），全局或局部调整密集点云。

- 推荐论文：
  - **《Non-Rigid Point Set Registration with Robust Transformation Learning under Manifold Regularization》 (TPAMI 2019)**
    提出基于流形正则化的非刚性配准方法，适用于矫正非线性误差。
  - **《Thin-Plate Spline Motion Model for Image Animation》 (CVPR 2020)**
    使用TPS进行图像变形，类似方法可迁移至点云矫正，通过控制点（稀疏真值）拟合全局变形。

------

### **3. 传感器融合与优化框架**

将稀疏真值作为约束，构建能量函数优化密集深度图，结合几何先验（如平滑性）。

- 推荐论文：
  - **《Guided Depth Enhancement via Anisotropic Diffusion》 (ICIP 2014)**
    基于各向异性扩散的深度图增强，结合稀疏真值引导优化，适用于传统方法实现。
  - **《LiDAR-Camera Fusion for Depth Completion and Enhancement》 (IROS 2020)**
    提出多模态融合框架，通过联合优化LiDAR与相机数据提升深度图质量。

------

### **4. 残差学习与深度矫正网络**

训练神经网络预测深度残差，直接以稀疏真值监督密集深度图的误差矫正。

- 推荐论文：
  - **《ResDepth: Learned Residual Surface Reconstruction》 (CVPR 2020)**
    使用残差学习优化三维表面重建，类似思路可迁移至深度残差预测。
  - **《Depth Error Correction via Weakly Supervised Learning》 (ECCV 2020 Workshop)**
    弱监督框架下，利用稀疏真值训练深度误差矫正网络。

------

### **5. 局部非线性模型与插值**

在稀疏点云覆盖的区域，拟合局部非线性模型（如多项式、RBF）调整密集深度值。

- 推荐论文：
  - **《Depth Super-Resolution via Local Linear Model and Non-Local Similarity》 (TIP 2017)**
    基于局部线性模型和非局部相似性的深度超分，可适配为局部误差矫正方法。
  - **《Radial Basis Function Networks for Depth Map Enhancement》 (ICANN 2016)**
    利用RBF网络从稀疏点云插值全局深度矫正场。

------

### **6. 伪激光雷达（Pseudo-LiDAR）相关改进**

除Pseudo++外，其他改进伪激光雷达生成质量的方法：

- 推荐论文：
  - **《LiDAR-aided Pseudo-LiDAR for 3D Object Detection》 (arXiv 2021)**
    利用LiDAR数据增强伪激光雷达点云，提升检测精度。
  - **《Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving》 (CVPR 2019)**
    原始伪激光雷达方法，可作为基线对比。

------

### **7. 自监督与半监督学习**

利用稀疏真值作为监督信号，结合自监督损失（如光度一致性）联合训练模型。

- 推荐论文：
  - **《Self-Supervised Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera》 (ICRA 2019)**
    结合自监督与稀疏监督，适用于动态场景深度补全。
  - **《Sparse-to-Dense Depth Estimation with Multi-Scale Guided Network》 (IEEE T-ITS 2021)**
    多尺度引导网络，融合稀疏深度与图像特征生成高质量密集深度。

------

### **实践建议**

- **步骤1：误差建模**
  分析密集深度误差的分布（如随距离变化的非线性），选择合适模型（多项式、神经网络）。
- **步骤2：选择矫正框架**
  若数据充足，可设计残差网络；若稀疏点分布均匀，可尝试TPS或RBF插值。
- **步骤3：联合优化**
  结合几何约束（如表面平滑性）与稀疏真值，构建能量函数优化密集深度图。

------

以上方法覆盖了深度学习、传统优化、传感器融合等多个方向，可根据实际场景和数据特点选择合适方案。







# 朝向



### **1. 基于局部-全局朝向分解的3D检测方法**

这类方法通过分离物体的全局朝向（如相对于相机坐标系）和局部朝向（如相对于物体自身坐标系），提升方向估计的鲁棒性。

- 推荐论文：
  - **《GS3D: An Efficient 3D Object Detection Framework for Autonomous Driving》 (CVPR 2019)**
    提出几何感知的单目3D检测框架，通过显式建模物体的局部朝向（如车头方向）和全局旋转，结合2D-3D几何约束优化检测结果。
  - **《RTM3D: Real-time Monocular 3D Detection from Object Keypoints》 (ECCV 2020)**
    通过预测物体关键点和几何约束（如射影角），解耦全局姿态与局部方向，实现实时单目3D检测。
  - **《Kinematic 3D Object Detection in Monocular Video》 (NeurIPS 2020)**
    引入运动学模型，将物体姿态分解为全局运动轨迹和局部朝向变化，结合时序信息优化检测结果。

------

### **2. 结合深度估计与方向矫正**

在生成密集深度图或伪点云时，通过局部-全局朝向分解提升几何一致性，常用于单目或双目深度估计任务。

- 推荐论文：
  - **《MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships》 (CVPR 2020)**
    利用物体间的相对位置和方向约束（如全局朝向一致性），结合深度估计提升单目检测精度。
  - **《CaDDN: Categorical Depth Distribution Network for Monocular 3D Object Detection》 (CVPR 2021)**
    通过预测深度分布并联合优化物体朝向（局部与全局分解），增强检测的几何鲁棒性。
  - **《D4LCN: Learning Depth-Guided Convolutions for Monocular 3D Object Detection》 (CVPR 2020)**
    将深度图与局部朝向估计结合，通过深度引导的卷积网络提升方向预测精度。

------

### **3. 多传感器融合中的方向矫正**

利用LiDAR或稀疏点云提供的真值，矫正密集伪点云的局部/全局朝向误差。

- 推荐论文：
  - **《LiDAR-RCNN: An Efficient Framework for 3D Object Detection using LiDAR Point Clouds》 (CVPR 2021)**
    提出基于LiDAR点云的局部朝向优化模块，通过稀疏真值引导密集预测的几何矫正。
  - **《Pseudo-Stereo for Monocular 3D Object Detection in Autonomous Driving》 (CVPR 2022)**
    在伪点云生成中引入局部表面法线估计（与朝向相关），结合稀疏LiDAR数据优化全局姿态。

------

### **4. 射影角（Observation Angle）的扩展应用**

射影角（物体相对于相机的视角方向）常与局部-全局朝向联合建模，解决方向模糊性问题。

- 推荐论文：
  - **《SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation》 (CVPRW 2020)**
    通过关键点预测和射影角估计，解耦物体方向，避免复杂的全局-局部分解计算。
  - **《MonoFlex: Towards Accurate Monocular 3D Object Detection via Adaptive Sample Fusion》 (CVPR 2021)**
    提出自适应样本融合策略，显式建模射影角对局部朝向的影响，提升极端视角下的检测鲁棒性。

------

### **5. 几何约束与自监督学习**

在无真值监督时，通过几何一致性约束（如局部朝向与表面法线对齐）优化深度与姿态。

- 推荐论文：
  - **《GUPNet: Geometry Uncertainty Propagation Network for Monocular 3D Object Detection》 (ICCV 2021)**
    引入几何不确定性传播机制，联合优化深度估计和局部朝向，利用投影一致性约束降低误差。
  - **《AutoShape: Real-Time Shape-Aware Monocular 3D Object Detection》 (ICCV 2021)**
    通过物体形状先验（CAD模型）与局部朝向的联合优化，提升单目检测的几何精度。

------

### **6. 点云生成与矫正中的方向优化**

在伪点云生成后，通过局部-全局变换矫正朝向误差。

- 推荐论文：
  - **《Pseudo-LiDAR++: Accurate Depth for 3D Object Detection in Autonomous Driving》 (ICLR 2021)**
    Pseudo-LiDAR++的扩展工作，引入表面法线估计和局部朝向对齐，优化伪点云的几何结构。
  - **《Depth-Conditioned Dynamic Message Propagation for Monocular 3D Object Detection》 (CVPR 2021)**
    在生成伪点云时，通过动态消息传播网络（DMP）结合局部朝向信息，提升深度图与点云质量。

------

### **总结与建议**

1. **技术分类**：
   - **方向分解**：将物体姿态解耦为全局（相机坐标系）和局部（物体坐标系）方向。
   - **射影角建模**：显式建模物体相对于相机的视角方向，解决方向歧义性。
   - **几何约束**：利用表面法线、关键点投影等约束优化局部朝向。
2. **实验方向**：
   - 若需矫正密集伪点云的朝向，可结合非刚性配准（如ICP或TPS）与方向分解模型。
   - 在深度估计网络中嵌入局部朝向预测分支，联合优化深度与姿态。
3. **关键论文推荐**：
   - 单目检测：**MonoFlex (CVPR 2021)**、**GUPNet (ICCV 2021)**
   - 多模态融合：**Pseudo-LiDAR++ (ICLR 2021)**、**LiDAR-RCNN (CVPR 2021)**
   - 自监督：**AutoShape (ICCV 2021)**

这些方法可为你的稀疏点云矫正密集伪点云任务提供理论支持，尤其在处理非线性深度误差时，结合局部几何约束（如表面法线）和全局姿态优化可能更有效。





### **1. 视锥（Frustum）与局部朝向结合的检测方法**

- **FQNet (CVPR 2019)**
  **《FQNet: Deep Fitting Degree Scoring Network for Monocular 3D Object Detection》**
  该方法通过将2D检测框扩展为3D视锥（Frustum），并利用局部几何特征预测物体朝向。FQNet显式建模了局部坐标系下的物体朝向，通过射向角分解优化全局姿态，与Mousavian的几何约束思想一脉相承。
- **Frustum PointNets (CVPR 2018)**
  **《Frustum PointNets for 3D Object Detection from RGB-D Data》**
  结合RGB图像生成的2D检测框构建3D视锥，并在视锥内使用点云网络（PointNet）预测物体姿态。虽然没有直接使用局部/全局朝向分解，但其对局部视锥空间内物体姿态的优化隐含了类似的局部几何约束。

------

### **2. 深度估计与伪点云（Pseudo-LiDAR）中的朝向矫正**

- **MonoGRNet (CVPR 2019)**
  **《Monocular 3D Object Detection with Geometric Constraints Embedding》**
  通过单目深度估计生成伪点云，结合几何约束预测3D框。其朝向预测模块将全局朝向分解为射向角和局部朝向角，直接沿用了Mousavian的分解策略，并利用深度图优化全局姿态。
- **PatchNet (ECCV 2020)**
  **《PatchNet: A Simple Deep Learning Pipeline for Long-Tail 3D Recognition》**
  基于单目深度估计生成伪点云，并通过局部补丁（Patch）特征预测物体朝向。该方法强调了局部几何特征对朝向估计的重要性，与局部朝向矫正的思想一致。
- **MonoFlex (CVPR 2021)**
  **《MonoFlex: Monocular 3D Object Detection with Camera-LiDAR Fusion via Dense Continuous Depth Estimation》**
  结合深度估计和关键点预测，显式建模物体局部朝向的歧义性（如对称性问题），并通过多模态融合优化全局朝向。

------

### **3. 基于局部-全局分解的端到端方法**

- **GUPNet (ICCV 2021)**
  **《Geometry Uncertainty Projection Network for Monocular 3D Object Detection》**
  提出几何不确定性投影网络，将全局朝向分解为局部几何特征（如关键点偏移）和相机参数投影，通过不确定性建模缓解深度估计误差对朝向的影响。
- **CaDDN (CVPR 2021)**
  **《Categorical Depth Distribution Network for Monocular 3D Object Detection》**
  通过分类深度分布生成伪点云，并在点云空间中使用局部几何特征预测物体姿态。其朝向预测模块结合了局部表面法向量和全局相机坐标系下的投影约束。

------

### **4. 射向角（Observation Angle）的显式优化**

- **Kinematic 3D (CVPR 2020)**
  **《Kinematic 3D Object Detection in Monocular Video》**
  在视频序列中引入运动学约束，将射向角与物体运动轨迹结合，通过时序信息优化全局朝向的稳定性。
- **D4LCN (CVPR 2020)**
  **《Depth-aware CNN for RGB-D Segmentation》**
  虽然不是3D检测工作，但其深度感知卷积（Depth-aware CNN）启发了后续方法（如MonoFlex）在局部特征提取中显式融合深度信息，间接支持了局部朝向的精确预测。

------

### **核心概念关联**

- **局部朝向**：通常指物体相对于相机射向角（Observation Angle）的局部旋转，与物体在图像平面中的表观形状强相关。
- **全局朝向**：物体在3D世界坐标系下的绝对旋转，需结合相机参数和射向角计算。
- **视锥（Frustum）**：基于2D检测框扩展的3D搜索空间，用于限制局部几何优化范围。
- **伪点云**：通过单目深度估计生成的仿LiDAR点云，提供局部几何先验以辅助朝向预测。

------

### **总结**

后续工作主要在以下方向扩展了局部-全局朝向分解思想：

1. **深度估计与伪点云**：通过深度图生成点云，结合局部几何特征优化朝向（如MonoGRNet、PatchNet）。
2. **视锥约束**：在受限的3D空间内预测局部朝向（如Frustum PointNets、FQNet）。
3. **对称性建模**：针对局部朝向歧义性（如车辆对称性），设计多任务损失函数（如MonoFlex）。
4. **时序融合**：利用视频序列中的运动信息稳定全局朝向（如Kinematic 3D）。

这些方法均继承了局部-全局分解的思想，并通过融合深度、几何或时序信息进一步提升了3D检测的精度。
