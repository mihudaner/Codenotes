# Ascend的aclgraph（一）aclgraph是什么？torchair又是怎么成图的？

```python
def _npu_backend:
    compiler = get_compiler(compiler_config)

def get_compiler:
    return _NpuFxCompiler(compiler_config)


class _NpuFxcompile():
    def __call__:
        return self._get_compiled_gm(gm, example_inputs)


def _get_compiled_gm：
	return _GmRunner(self._gen_compiled_gm(gm, example_inputs))

def _gen_compiled_gm:
     with no_dispatch():
            mutable_gm = copy.deepcopy(gm)
            
     graph = AclConcreteGrap
        
    concrete_graph: ConcreteGraphBase = _NpuGraphConverter(
                mutable_gm, graph=graph, garbage_collect_values=False).run(*example_inputs)

    concrete_graph.optimize_graph_without_runtime(*example_inputs)
    return concrete_graph
```

在_NpuGraphConverter中调用run方法，该方法的作用：从GeConcreteGraph或者AclConcreteGraph返回的graph中，通过相关pass对图进行进一步修改，当前只有一个pass : _optimize_sym_input，作用是对图的输入进行call_function的调用，用新的node去替换。



```python
class _NpuGraphConverter:
	def run():
        optimized_fx = _optimize_fx(self.module)
        
def _optimize_fx：
	graph_module = _optimize_sym_input(graph_module)

def _optimize_sym_input(graph_module: torch.fx.GraphModule):
   
    for sym_node in sym_input_list:
        for tensor_node in tensor_input_list:
       
            for i in range(len(tensor_node.meta['val'].size())):
                # find sym node is a dim of other fake tensor, replace it.
                with graph_module.graph.inserting_after(tensor_node):
                    sym_size_node = graph_module.graph.create_node(op="call_function", 					    					target=torch.ops.aten.sym_size, args=(tensor_node, i))

```





concrete_graph.optimize_graph_without_runtime(*example_inputs)进行图的优化。

```python
 concrete_graph.optimize_graph_without_runtime(*example_inputs)
    
 class AclConcreteGraph(ConcreteGraphBase):
    def optimize_graph_without_runtime(self, *sample_args)
```

optimize_graph_without_runtime是在GeConcreteGraph和AclConcreteGraph中都有被定义，而且这2个graph都是继承自ConcreteGraphBase。

此处concrete_graph的被赋值是ConcreteGraphBase类型，因此optimize_graph_without_runtime会自动选择GE下的还是ACL下的优化，也就类似与C++中的虚函数。



最终：

这里就是实例化了_GmRunner一个对象，最终会调用其__call__方法，__call__中又会调用self.runner，该runner就是GeConcreteGraph或者AclConcreteGraph生成图的graph。也就是说，这里开始执行图了。



```python
class _GmRunner:
    def __init__(self, runner: Callable):
        self.runner = runner
    def __call__(self, *args, **kwargs):
        with record_function("npu_fx_compiler inference"):
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug('runtime inputs')
                for i, inp in enumerate(args):
                    logger.debug('  input %s: %s', i, _summary(inp))
                for k, v in kwargs.items():
                    logger.debug('  input %s: %s', k, _summary(v))

            gm_result = self.runner(*args, **kwargs)

            if logger.isEnabledFor(logging.DEBUG):
                logger.debug('runtime outputs')
                for i, inp in enumerate(gm_result):
                    logger.debug('  output %s: %s', i, _summary(inp))

            return gm_result
```
```python

class AclConcreteGraph(ConcreteGraphBase):
    def __call__(self, *args: Any, **kwargs: Any) -> Any:
            """
            Executes the compiled ACL graph with the provided inputs.

            This method handles input processing, graph execution, and output retrieval.
            It ensures proper data synchronization between captured inputs and user-provided inputs
            for in-place operations that may modify tensor addresses.

            Args:
                *args: Variable length argument list for graph inputs.
                **kwargs: Arbitrary keyword arguments for graph inputs.

            Returns:
                Any: Output tensors from the executed graph.
            """        
            # get graph_key and capture
            fn_key = self.compile(*args, **kwargs)

            # input process
            self.graph.process_input(fn_key, *args)

            # run/replay
            with record_function("acl_graph_replay"):
                self.graph.run(fn_key, *args, **kwargs)

            # For in-place op, dynamo will transform it into a functionalized call and add copy_ node when setting
            # keep_inference_input_mutations=True, which may need data copy from capture input to user input (when tensor
            # address is different between capture and replay).
            self.graph.process_inplace_inputs(fn_key, *args)

            return self.graph.reconstruct_outputs(fn_key)
```





### 1. 算子如何被 “记录” 在计算图中？

`convert_print` 运行在**静态图构建阶段**，其本质是通过修改 “计算图数据结构” 来记录算子：



- 当 `convert_print` 被调用时，它会通过 `ge.StringFormat()` 和 `ge.PrintV2()` 生成 NPU 硬件可识别的算子对象。

- 这些算子会被**显式添加到当前的 GE 计算图中**（通过 `get_default_ge_graph().op` 操作，代码中 `print_op = get_default_ge_graph().op[-1]` 就是获取刚添加的算子）。

- 最终形成的 GE 计算图是一个包含所有算子（包括打印算子）的完整数据结构，会被序列化存储（如保存为模型文件或内存中的图对象）。

  ```python
  def register_fx_node_ge_converter(aten_op):
      """
      Registers a converter for a PyTorch operation.
  
      Args:
          aten_op: The PyTorch operation to register.
  
      Returns:
          Callable: Empty function if aten_op is None, else the Converter instance.
      """    
      if aten_op is None:
          return empty_function
      return Converter(aten_op)
      
  class Converter:
      """
      Base class for converting PyTorch operations to GE operations.
      """    
      compile_backend = None
      result_checker = None
  
      def __init__(self, aten_op) -> None:
          self._aten_op = aten_op
          self._signature = inspect.signature(aten_op)
          self._supported_cases = None
  
      def __call__(self, converter) -> Any:
          """
          Registers a converter function for the associated PyTorch operation.
  
          Args:
              converter (Callable): The converter function to register.
  
          Returns:
              Any: The registered converter.
          """        
          wrapped_converter = _wrap_converter(converter)
          if 'meta_outputs' in inspect.signature(converter).parameters:
              wrapped_converter.require_meta = True
          else:
              wrapped_converter.require_meta = False
          try:
              self._aten_op._ge_converter = wrapped_converter
          except Exception:
              global _CONVERTERS
              _CONVERTERS.update({self._aten_op: wrapped_converter})
          return self
  ```

  _CONVERTERS注册表

1. **告知框架触发时机**：当框架在 FX 图中遇到 `torch.ops.air.print.default` 算子节点时，会自动从注册表中找到对应的 `convert_print` 函数并调用。



### 为什么无需调用 `convert_print` 就能完成注册？

装饰器的本质是 “语法糖”，它的执行依赖于**函数定义**而非**函数调用**。例如：

```
def decorator(func):
    print("装饰器执行了")
    return func

@decorator
def my_func():
    pass

# 输出：装饰器执行了（此时 my_func 尚未被调用）
```

同理，`@register_fx_node_ge_converter(...)` 会在 `convert_print` 函数定义时自动触发注册逻辑，无论 `convert_print` 是否被调用，映射关系都会被记录。

### 总结

- **注册时机**：`register_fx_node_ge_converter` 作为装饰器，在**模块加载、函数定义时**自动执行，完成算子与转换器的映射注册。







# Ascend的aclgraph（二）_npu_backend中还有些什么秘密？

上文[Ascend的aclgraph（一）aclgraph是什么？torchair又是怎么成图的？](https://blog.csdn.net/xyz3120/article/details/147814982?sharetype=blogdetail&sharerId=147814982&sharerefer=PC&sharesource=xyz3120&spm=1011.2480.3001.8118)对`get_compiler`函数进行了分析。本章接着分析剩余的4个部分。