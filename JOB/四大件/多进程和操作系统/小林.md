# 前言



# 硬件结构

## SSD/HDD 硬盘

SSD（*Solid-state disk*） 就是我们常说的固体硬盘，结构和内存类似，但是它相比内存的优点是断电后数据还是存在的，而内存、寄存器、高速缓存断电后数据都会丢失。内存的读写速度比 SSD 大概快 `10~1000` 倍。

当然，还有一款传统的硬盘，也就是机械硬盘（*Hard Disk Drive, HDD*），它是通过物理读写的方式来访问数据的，因此它访问速度是非常慢的，它的速度比内存慢 `10W` 倍左右。

由于 SSD 的价格快接近机械硬盘了，因此机械硬盘已经逐渐被 SSD 替代了。

## CPU缓存一致性

### CPU Cache 的数据写入

随着时间的推移，CPU 和内存的访问性能相差越来越大，于是就在 CPU 内部嵌入了 CPU Cache（高速缓存），CPU Cache 离 CPU 核心相当近，因此它的访问速度是很快的，于是它充当了 CPU 与内存之间的缓存角色。

CPU Cache 通常分为三级缓存：L1 Cache、L2 Cache、L3 Cache，级别越低的离 CPU 核心越近，访问速度也快，但是存储容量相对就会越小。其中，在多核心的 CPU 里，每个核心都有各自的 L1/L2 Cache，而 L3 Cache 是所有核心共享使用的。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84/CPU-Cache.png)

我们当然期望 CPU 读取数据的时候，都是尽可能地从 CPU Cache 中读取，而不是每一次都要从内存中获取数据。

#### 写直达

保持内存与 Cache 一致性最简单的方式是，**把数据同时写入内存和 Cache 中**，这种方法称为**写直达（\*Write Through\*）**。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E5%86%99%E7%9B%B4%E8%BE%BE.png)

在这个方法里，写入前会先判断数据是否已经在 CPU Cache 里面了：

- 如果数据已经在 Cache 里面，先将数据更新到 Cache 里面，再写入到内存里面；
- 如果数据没有在 Cache 里面，就直接把数据更新到内存里面。

写直达法很直观，也很简单，但是问题明显，无论数据在不在 Cache 里面，每次写操作都会写回到内存，这样写操作将会花费大量的时间，无疑性能会受到很大的影响。

#### [写回](https://www.xiaolincoding.com/os/1_hardware/cpu_mesi.html#写回)

既然写直达由于每次写操作都会把数据写回到内存，而导致影响性能，于是为了要减少数据写回内存的频率，就出现了**写回（\*Write Back\*）的方法**。

在写回机制中，**当发生写操作时，新的数据仅仅被写入 Cache Block 里，只有当修改过的 Cache Block「被替换」时才需要写到内存中**，减少了数据写回内存的频率，这样便可以提高系统的性能。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E5%86%99%E5%9B%9E1.png)

那具体如何做到的呢？下面来详细说一下：

- 如果当发生写操作时，数据已经在 CPU Cache 里的话，则把数据更新到 CPU Cache 里，同时标记 CPU Cache 里的这个 Cache Block 为脏（Dirty）的，这个脏的标记代表这个时候，我们 CPU Cache 里面的这个 Cache Block 的数据和内存是不一致的，这种情况是不用把数据写到内存里的；
- 如果当发生写操作时，数据所对应的 Cache Block 里存放的是「别的内存地址的数据」的话，就要检查这个 Cache Block 里的数据有没有被标记为脏的：
  - 如果是脏的话，我们就要把这个 Cache Block 里的数据写回到内存，然后再把当前要写入的数据，先从内存读入到 Cache Block 里（注意，这一步不是没用的，具体为什么要这一步，可以看这个「[回答 (opens new window)](https://stackoverflow.com/questions/26672661/for-write-back-cache-policy-why-data-should-first-be-read-from-memory-before-w)」），然后再把当前要写入的数据写入到 Cache Block，最后也把它标记为脏的；
  - 如果不是脏的话，把当前要写入的数据先从内存读入到 Cache Block 里，接着将数据写入到这个 Cache Block 里，然后再把这个 Cache Block 标记为脏的就好了。

## 缓存一致性

如果这时旁边的 B 号核心尝试从内存读取 i 变量的值，则读到的将会是错误的值，因为刚才 A 号核心更新 i 值还没写入到内存中，内存中的值还依然是 0。**这个就是所谓的缓存一致性问题，A 号核心和 B 号核心的缓存，在这个时候是不一致，从而会导致执行结果的错误。**

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E9%97%AE%E9%A2%98%E4%BE%8B%E5%AD%902.png)

那么，要解决这一问题，就需要一种机制，来同步两个不同核心里面的缓存数据。要实现的这个机制的话，要保证做到下面这 2 点：

- 第一点，某个 CPU 核心里的 Cache 数据更新时，必须要传播到其他核心的 Cache，这个称为**写传播（\*Write Propagation\*）**；
- 第二点，某个 CPU 核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为**事务的串行化（\*Transaction Serialization\*）**。

第一点写传播很容易就理解，当某个核心在 Cache 更新了数据，就需要同步到其他核心的 Cache 里。而对于第二点事务的串行化，我们举个例子来理解它。

假设我们有一个含有 4 个核心的 CPU，这 4 个核心都操作共同的变量 i（初始值为 0 ）。A 号核心先把 i 值变为 100，而此时同一时间，B 号核心先把 i 值变为 200，这里两个修改，都会「传播」到 C 和 D 号核心。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/CPU%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7/%E4%BA%8B%E4%BB%B6%E9%A1%BA%E5%BA%8F%E9%97%AE%E9%A2%98.png)

那么问题就来了，C 号核心先收到了 A 号核心更新数据的事件，再收到 B 号核心更新数据的事件，因此 C 号核心看到的变量 i 是先变成 100，后变成 200。

而如果 D 号核心收到的事件是反过来的，则 D 号核心看到的是变量 i 先变成 200，再变成 100，虽然是做到了写传播，但是各个 Cache 里面的数据还是不一致的。

所以，我们要保证 C 号核心和 D 号核心都能看到**相同顺序的数据变化**，比如变量 i 都是先变成 100，再变成 200，这样的过程就是事务的串行化。

要实现事务串行化，要做到 2 点：

- CPU 核心对于 Cache 中数据的操作，需要同步给其他 CPU 核心；
- 要引入==「锁」==的概念，如果两个 CPU 核心里有相同数据的 Cache，那么对于这个 Cache 数据的更新，只有拿到了「锁」，才能进行对应的数据更新。

# 软中断

### 中断描述符表要写入中断控制器么？

> 不需要。中断控制器负责接收和处理硬件中断信号，并将中断请求发送给CPU。==而中断描述符表是CPU内部的一部分==，它包含了中断处理程序的地址和属性，当中断发生时，CPU会根据IDT中的信息跳转到相应的中断处理程序。
>
> 中断控制器（如PIC、APIC）主要是管理中断的优先级、屏蔽和分发中断信号，而中断描述符表则由操作系统在初始化阶段配置好并加载到CPU的相关寄存器中。
>

### 中断控制器怎么利用中断描述符表（IDT）获得地址的

> ChatGPT
>
> 当中断控制器（例如PIC或APIC）接收到硬件中断并将其发送给CPU时，CPU会进行以下步骤来处理该中断并利用中断描述符表（IDT）获得中断处理程序的地址：
>
> 1. **识别中断向量**：中断控制器发送一个中断请求信号给CPU，并提供一个中断向量（Interrupt Vector）。这个中断向量是一个数字，用来标识发生了哪个中断。
> 2. **索引IDT**：CPU使用中断向量作为索引，查找中断描述符表（IDT）中的相应条目。IDT是一个包含256个条目的数组，每个条目描述一个中断或异常的处理程序。
> 3. **获取中断处理程序地址**：每个IDT条目包含中断处理程序的地址（分为高16位和低16位）和其他一些信息（如段选择子、特权级别等）。CPU读取这个地址信息，从而知道该中断对应的中断处理程序的位置。
> 4. **跳转到中断处理程序**：根据IDT条目中的信息，CPU跳转到相应的中断处理程序执行。此时，CPU会保存当前的执行状态，以便中断处理程序完成后能够恢复原来的执行状态。
>
> 以下是一个简化的示例，描述了IDT条目结构（假设为32位模式）：
>
> ```
> c复制代码struct IDTEntry {
>     uint16_t offset_low;  // 中断处理程序地址的低16位
>     uint16_t selector;    // 代码段选择子
>     uint8_t  zero;        // 保留，置为0
>     uint8_t  type_attr;   // 类型和属性
>     uint16_t offset_high; // 中断处理程序地址的高16位
> };
> ```
>
> 假设中断向量是`n`，对应IDT的第`n`个条目，那么CPU通过以下步骤获取中断处理程序的地址：
>
> ```
> c复制代码IDTEntry entry = IDT[n];
> uint32_t handler_address = (entry.offset_high << 16) | entry.offset_low;
> ```
>
> 然后，CPU跳转到`handler_address`处执行中断处理程序。
>
> 



### CPU怎么找到中断描述符表（IDT）的

CPU找到中断描述符表（IDT）的过程是通过一个特殊的寄存器来实现的，这个寄存器叫做中断描述符表寄存器（IDTR）



### 中断描述符表寄存器（IDTR）又是怎么找到的

> ChatGPT
>
> 中断描述符表寄存器（IDTR）是CPU内部的一个特殊寄存器，它的存在和使用是由CPU的硬件设计和体系结构决定的。以下是关于IDTR的详细说明：
>
> 
>
> 总结来说，==IDTR是CPU硬件设计的一部分，始终存在于CPU内部==。操作系统在启动和初始化过程中通过`lidt`指令设置IDTR的值。当中断发生时，CPU自动使用IDTR来查找和访问IDT，从而跳转到正确的中断处理程序。
>
> 
> 
> 以下是详细过程：
> 
> 1. **中断发生**：
>   - 当硬件中断发生时，中断控制器（如PIC或APIC）向CPU发送一个中断请求（IRQ）。
>    - CPU响应中断请求，保存当前的状态，包括程序计数器（EIP/RIP）、标志寄存器（EFLAGS/RFLAGS）等。
>2. **中断向量**：
>    - CPU接收到中断请求后，从中断控制器获取中断向量（一个标识具体中断类型的数字）。
> 3. **查找IDT**：
>    - CPU使用中断向量作为索引，从IDTR（中断描述符表寄存器）中获取IDT的基地址和限制。
>    - 根据中断向量和IDT的基地址，计算出对应的IDT条目的地址。
> 4. **读取IDT条目**：
>       - CPU从IDT中读取对应条目，获取中断处理程序的地址及相关信息。
> 5. **跳转到中断处理程序**：
>    - CPU根据IDT条目中的地址跳转到中断处理程序执行。
> 
> 这一系列操作完全在硬件层面进行，不需要操作系统代码的介入。操作系统的职责是在系统启动时配置和加载IDT（使用`lidt`指令），并为每个中断向量提供适当的中断处理程序。中断发生时，硬件自动处理上下文切换和中断处理程序的调用。
>
> 所以，中断处理的触发和初步处理（查找IDT和跳转到中断处理程序）是由CPU硬件完成的，而具体的中断处理逻辑（中断处理程序的代码）是由操作系统提供和编写的。
>

### 那软中段是怎么实现的

> 软中断（Software Interrupt）是通过软件指令触发的中断，不需要依赖硬件中断控制器。它们常用于操作系统内核与应用程序之间的交互，以及在内核内部进行特定任务。以x86架构为例，软中断主要通过`int`指令实现。
>
> 以下是软中断的实现过程：
>
> ### 1. 软中断指令
>
> 在x86架构上，`int`指令用于触发软中断。指令格式如下：
>
> ```
>assembly
> 复制代码
> int n
> ```
> 
> 其中`n`是中断向量号，它指定了要触发的中断类型。
>
> ### 2. CPU处理软中断的步骤
>
> 当CPU执行`int n`指令时，会执行以下步骤：
>
> 1. **保存上下文**：
>
>    - CPU保存当前的执行状态，包括程序计数器（EIP/RIP）、标志寄存器（EFLAGS/RFLAGS）、段寄存器等。
>
> 2. **查找IDT**：
>
>    - CPU使用中断向量`n`作为索引，从IDTR（中断描述符表寄存器）中获取IDT的基地址和限制。
>   - 计算出对应的IDT条目的地址，例如：`IDT_base + n * 8`（在32位模式下，每个IDT条目占8字节）。
> 
> 3. **读取IDT条目**：
>
>    - CPU从IDT中读取对应条目，获取中断处理程序的地址及相关信息。
>
>    - IDT条目结构如下：
>
>      ```
>     c复制代码struct IDTEntry {
>          uint16_t offset_low;  // 中断处理程序地址的低16位
>          uint16_t selector;    // 代码段选择子
>          uint8_t  zero;        // 保留，置为0
>          uint8_t  type_attr;   // 类型和属性
>          uint16_t offset_high; // 中断处理程序地址的高16位
>      };
>      ```
> 
>    - 计算中断处理程序的地址：`handler_address = (entry.offset_high << 16) | entry.offset_low`
>
> 4. **跳转到中断处理程序**：
>
>    - 根据从IDT条目中读取的地址，CPU跳转到相应的中断处理程序执行。
>
> ### 3. 中断处理程序
>
> 软中断处理程序是操作系统提供的函数，用于处理特定的中断请求。它的职责包括：
>
> - 执行与中断相关的任务，例如系统调用处理、调度、异常处理等。
>- 恢复被中断的任务状态，使得任务可以继续执行。
> 
> ### 4. 返回中断处理程序
>
> 中断处理程序完成后，使用`iret`指令返回到被中断的代码执行。`iret`指令会恢复之前保存的执行状态，包括程序计数器和标志寄存器等。
>
> ### 示例
>
> 以下是一个简单的软中断例子，用于调用中断向量0x80（通常用于Linux系统调用）：
>
> ```
>assembly复制代码section .text
> global _start
> 
> _start:
>     ; 触发0x80软中断
>     mov eax, 1      ; 系统调用号1：sys_exit
>     xor ebx, ebx    ; 返回码0
>     int 0x80        ; 触发0x80中断（Linux系统调用）
> ```
> 
> 这个程序通过触发0x80中断来调用Linux内核的`sys_exit`系统调用。
>
> 总之，软中断通过软件指令触发，中断处理的初始过程（如查找IDT和跳转到中断处理程序）由CPU硬件自动处理，而具体的中断处理逻辑由操作系统提供的中断处理程序实现。
>
> 如果用c++写内核程序怎么触发一个软中断
>
> ChatGPT
>
> 在C++内核程序中，触发软中断通常需要使用内联汇编，因为C++本身并不提供直接触发中断的关键字或函数。以下是一个例子，展示如何在C++内核代码中触发一个软中断：
>
> ### 示例代码
>
> 假设我们要触发中断向量`0x80`，这在Linux系统中通常用于系统调用。以下是一个简单的示例：
>
> ```
>cpp复制代码#include <cstdint>
> 
> extern "C" void trigger_soft_interrupt(uint8_t interrupt_vector) {
>     asm volatile (
>         "int %0"
>         :
>         : "i" (interrupt_vector)
>     );
> }
> 
> int main() {
>     trigger_soft_interrupt
> ```
> 
> 

### 汇编触发软中段的的伪代码

> ChatGPT
>
> 以下是用汇编语言触发软中断的伪代码，并在C++内核程序中使用内联汇编来实现触发软中断的示例。
>
> ### 汇编触发软中断的伪代码
>
> ```
> assembly复制代码; 假设我们要触发中断向量 n
> int n  ; 使用 `int` 指令触发软中断
> ```
>
> ### C++ 内联汇编触发软中断的示例
>
> 以下是如何在C++代码中使用内联汇编来触发一个软中断的示例：
>
> ```
> cpp复制代码#include <cstdint>
> 
> // 函数声明
> extern "C" void trigger_soft_interrupt(uint8_t interrupt_vector);
> 
> extern "C" void trigger_soft_interrupt(uint8_t interrupt_vector) {
>     // 使用内联汇编触发软中断
>     asm volatile (
>         "int %0"          // `int` 指令触发中断
>         :                 // 没有输出操作数
>         : "i" (interrupt_vector)  // 输入操作数，`interrupt_vector` 表示中断向量号
>         : "memory"        // 通知编译器该指令可能会修改内存
>     );
> }
> 
> int main() {
>     // 触发中断向量 0x80 (通常用于系统调用)
>     trigger_soft_interrupt(0x80);
>     return 0;
> }
> ```
>
> ### 解释
>
> 1. **声明函数**：
>
>    ```
>    cpp
>    复制代码
>    extern "C" void trigger_soft_interrupt(uint8_t interrupt_vector);
>    ```
>
> 2. **实现函数**：
>
>    ```
>    cpp复制代码extern "C" void trigger_soft_interrupt(uint8_t interrupt_vector) {
>        asm volatile (
>            "int %0"          // `int` 指令触发中断
>            :                 // 没有输出操作数
>            : "i" (interrupt_vector)  // 输入操作数，`interrupt_vector` 表示中断向量号
>            : "memory"        // 通知编译器该指令可能会修改内存
>        );
>    }
>    ```
>
> 3. **调用函数**：
>
>    ```
>    cpp复制代码int main() {
>        trigger_soft_interrupt(0x80);
>        return 0;
>    }
>    ```
>
> 这个代码示例展示了如何在C++内核程序中使用内联汇编触发一个软中断。通过调用`trigger_soft_interrupt`函数，并传递中断向量号（例如`0x80`），可以触发相应的软中断。这在操作系统内核编程中非常有用，特别是在实现系统调用等功能时。
>

### 不用实现保护现场什么的么？

> 在实际的操作系统内核编程中，触发软中断时确实需要保护和恢复CPU的上下文（现场），但这些操作通常是由中断处理程序（ISR）来完成的，而不是由触发软中断的代码负责。触发软中断的代码只需要简单地发出一个`int`指令即可。
>
> ### 触发软中断的代码（C++内联汇编）
>
> ```
>cpp复制代码#include <cstdint>
> 
>// 函数声明
> extern "C" void trigger_soft_interrupt(uint8_t interrupt_vector);
> 
> extern "C" void trigger_soft_interrupt(uint8_t interrupt_vector) {
>  asm volatile (
>      "int %0"          // `int` 指令触发中断
>      :                 // 没有输出操作数
>      : "i" (interrupt_vector)  // 输入操作数，`interrupt_vector` 表示中断向量号
>         : "memory"        // 通知编译器该指令可能会修改内存
>     );
>    }
>    
>    int main() {
>     // 触发中断向量 0x80 (通常用于系统调用)
>  trigger_soft_interrupt(0x80);
>  return 0;
> }
>    ```
>    
>    ### 中断处理程序（汇编）
> 
> 假设这是你的中断处理程序（ISR），需要保存和恢复CPU上下文。这部分代码通常是用汇编语言编写的，因为它需要直接访问CPU寄存器。
>
> ```
>assembly复制代码global isr_handler
> 
>isr_handler:
>  ; 保存通用寄存器的现场
>  pusha
>  push ds
>  push es
>     push fs
>     push gs
>    
>     ; 加载内核数据段选择子
>     mov ax, 0x10
>     mov ds, ax
>  mov es, ax
>     mov fs, ax
>     mov gs, ax
>    
>     ; 调用实际的中断处理程序（C 函数）
>     call c_isr_handler
>    
>  ; 恢复通用寄存器的现场
>     pop gs
>     pop fs
>  pop es
>     pop ds
>     popa
>    
>     ; 使用 iret 指令返回
>     iret
>    ```
> 
>    ### 实际的中断处理程序（C++）
>    
> 这是你的中断处理程序，它会在中断发生时被调用
>
> ```
>cpp复制代码extern "C" void c_isr_handler() {
>  // 在这里处理中断
> // 例如，处理系统调用，调度任务等
> }
> ```
>    

### 软中断内核线程

> 还有一个区别，硬中断（上半部）是会打断 CPU 正在执行的任务，然后立即执行中断处理程序，而软中断（下半部）是以内核线程的方式执行，并且每一个 CPU 都对应一个软中断内核线程，名字==通常为「ksoftirqd/CPU 编号」==，比如 0 号 CPU 对应的软中断内核线程的名字是 ksoftirqd/0，这个软中断内核线程又是起什么作用
>
> 
>
> 你说的区别是准确的。硬中断和软中断在处理方式上确实有所不同，尤其是在Linux内核中，==这两种中断处理机制被清晰地划分为上半部和下半部==。
>
> ### 硬中断（上半部）
>
> 硬中断处理程序（ISR）是由硬件触发的，当硬中断发生时，CPU立即暂停当前正在执行的任务，转而执行ISR。这部分处理程序必须尽可能短小精悍，以确保系统的实时响应性。通常，ISR只负责最基本的和时间紧迫的工作，比如：
>
> - 硬件设备的状态确认。
> - 读取或写入数据缓冲区。
> - 安排需要进一步处理的任务。
>
> 因为硬中断处理程序是在中断上下文中运行的，所以它不能进行任何会阻塞的操作，如等待I/O，分配内存等。
>
> ### 软中断（下半部）
>
> 软中断处理程序负责完成硬中断处理程序未完成的工作，这些工作可以稍微延迟且不需要立即处理。软中断通常以内核线程的方式执行。在Linux内核中，软中断的处理通过软中断内核线程`ksoftirqd`来实现。
>
> ### 软中断内核线程 `ksoftirqd`
>
> 每个CPU对应一个软中断内核线程，名字通常为`ksoftirqd/CPU编号`，例如0号CPU对应的线程为`ksoftirqd/0`。这些线程的主要作用是：
>
> 1. **处理软中断**：`ksoftirqd`线程主要处理被硬中断处理程序或其他内核代码标记的软中断。软中断包括任务队列、网络包处理等需要在稍后执行的任务。
> 2. **延迟处理**：通过将延迟处理的工作移到`ksoftirqd`线程，可以避免在硬中断处理程序中进行复杂的或长时间的处理，从而保持系统的高响应性。
> 3. **内核线程运行**：`ksoftirqd`线程在内核空闲时被调度运行，处理挂起的软中断。它们的优先级通常较低，以避免对正常任务调度造成过多影响。
>
> ### 软中断的工作流程
>
> 1. **硬中断触发**：当硬件设备触发一个硬中断时，CPU调用硬中断处理程序（ISR）。
> 2. **标记软中断**：在ISR中，必要时会标记软中断。例如，网络驱动程序在ISR中接收网络包后，会标记需要处理网络包的软中断。
> 3. **软中断调度**：内核调度器会在适当的时候调度`ksoftirqd`线程来处理标记的软中断。`ksoftirqd`线程会遍历所有挂起的软中断，并依次处理它们。
>
> ### 示例代码
>
> 以下是一个简化的示例，展示硬中断和软中断的基本工作流程：
>
> ```
> c复制代码#include <linux/interrupt.h>
> #include <linux/kernel.h>
> #include <linux/module.h>
> #include <linux/irq.h>
> 
> // 假设硬中断号为 1
> #define HARDIRQ_NUM 1
> 
> // 定义软中断
> void my_softirq_handler(struct softirq_action *action);
> 
> // 定义硬中断处理程序
> irqreturn_t my_isr(int irq, void *dev_id) {
>     // 处理硬中断（上半部）
>     printk(KERN_INFO "Hard IRQ: %d\n", irq);
> 
>     // 标记软中断
>     raise_softirq_irqoff(0);
> 
>     return IRQ_HANDLED;
> }
> 
> // 注册硬中断和软中断
> static int __init my_module_init(void) {
>     // 注册硬中断处理程序
>     if (request_irq(HARDIRQ_NUM, my_isr, IRQF_SHARED, "my_isr", NULL)) {
>         printk(KERN_ERR "Failed to register hard irq\n");
>         return -1;
>     }
> 
>     // 注册软中断处理程序
>     open_softirq(0, my_softirq_handler);
> 
>     return 0;
> }
> 
> // 注销硬中断和软中断
> static void __exit my_module_exit(void) {
>     free_irq(HARDIRQ_NUM, NULL);
> }
> 
> // 软中断处理程序
> void my_softirq_handler(struct softirq_action *action) {
>     printk(KERN_INFO "Soft IRQ handled\n");
> }
> 
> module_init(my_module_init);
> module_exit(my_module_exit);
> 
> MODULE_LICENSE("GPL");
> MODULE_AUTHOR("Example Author");
> MODULE_DESCRIPTION("Example module with hard and soft IRQ handling");
> ```
>
> ### 解释
>
> - **硬中断处理程序**：`my_isr`是硬中断处理程序，当硬中断发生时，它会被调用。`raise_softirq_irqoff`函数用于标记一个软中断，以便稍后由`ksoftirqd`线程处理。
> - **软中断处理程序**：`my_softirq_handler`是软中断处理程序，它会在`ksoftirqd`线程中被调用，处理挂起的软中断任务。
>
> 通过这种方式，硬中断和软中断协调工作，可以确保系统的高效和实时响应能力，同时避免在硬中断处理程序中进行复杂的处理。

# 内存管理



c++.md里写了一些部分是之前疑问查的博客，这里系统再看一下

## 虚拟内存

我们可以把进程所使用的地址「隔离」开来，即让操作系统为每个进程分配独立的一套「**虚拟地址**」，人人都有，大家自己玩自己的地址就行，互不干涉。但是有个前提每个进程都不能访问物理地址，至于虚拟地址最终怎么落到物理内存里，对进程来说是透明的，操作系统已经把这些都安排的明明白白了。

![进程的中间层](https://cdn.xiaolincoding.com//mysql/other/298fb68e3da94d767b02f2ed81ebf2c4.png)

**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

如果程序要访问虚拟地址的时候，由操作系统转换成不同的物理地址，这样不同的进程运行的时候，写入的是不同的物理地址，这样就不会冲突了。

于是，这里就引出了两种地址的概念：

- 我们程序所使用的内存地址叫做**虚拟内存地址**（*Virtual Memory Address*）
- 实际存在硬件里面的空间地址叫**物理内存地址**（*Physical Memory Address*）。
- https://www.bilibili.com/video/BV1ws4y1k7xp/?spm_id_from=333.999.0.0&vd_source=eef102f4fb053709a57c96d0c876628a

![image-20240407162140816](/home/wangkai/codenotes_ubuntu/小林/img/image-20240407162140816.png)

## 虚拟地址和分段分页



![image-20240708213840498](.\img\image-20240708213840498.png)



### &取到的地址是虚拟地址还是虚拟地址的偏移？

> 
>
> 在编程时，使用`&`运算符取到的地址是虚拟地址，而不是虚拟地址的偏移或物理地址。

### 怎么保证不同进程里相同的虚拟地址可以映射到不同的物理地址

> ，这是通过操作系统的虚拟内存管理机制实现的。这种机制使得每个进程拥有独立的地址空间，从而增强了系统的安全性和稳定性。以下是其实现原理：
>
> 虚拟内存管理机制
> 虚拟内存管理机制使用页表（page tables）来管理虚拟地址和物理地址的映射关系。==每个进程都有自己的页表==，记录该进程的虚拟地址到物理地址的映射。这意味着相同的虚拟地址在不同的进程中可以映射到不同的物理地址



> 假设我们有两个进程A和B，它们的页表分别如下：
>
> - **进程A的页表**：
>
> | 虚拟地址 | 物理地址 |
> | -------- | -------- |
> | 0x400000 | 0x100000 |
> | 0x400100 | 0x100100 |
>
> - **进程B的页表**：
>
> | 虚拟地址 | 物理地址 |
> | -------- | -------- |
> | 0x400000 | 0x200000 |
> | 0x400100 | 0x200100 |
>
> 即使进程A和进程B的虚拟地址0x400000相同，它们通过各自的页表映射到不同的物理地址0x100000和0x200000。



> 建立映射时确保不同进程不会映射到同一个物理内存地址，==是通过操作系统的内存管理子系统来实现的==。这个子系统负责分配、追踪和管理所有进程的物理内存。以下是具体机制和步骤：
>
> ### 内存管理子系统
>
> 1. **物理内存管理器**：
>    - 物理内存管理器==维护一个全局的物理内存分配==表，==跟踪每个物理内存块的使用情况==，确保不会将同一物理内存块分配给多个进程。
> 2. **内存分配算法**：
>    - 操作系统使用内存分配算法（如Buddy系统、Slab分配器等）来分配物理内存块。这些算法保证每次分配的物理内存块都是唯一且未被占用的

## 内存分页

分段的好处就是能产生连续的内存空间，但是会出现「外部内存碎片和内存交换的空间太大」的问题。

要解决这些问题，那么就要想出能少出现一些内存碎片的办法。另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，这样就可以解决问题了。这个办法，也就是**内存分页**（*Paging*）。

**分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小**。这样一个连续并且尺寸固定的内存空间，我们叫**页**（*Page*）。在 Linux 下，==每一页的大小为== `4KB`。

虚拟地址与物理地址之间通过**页表**来映射，如下图：

![img](https://cdn.xiaolincoding.com//mysql/other/08a8e315fedc4a858060db5cb4a654af.png)

页表是存储在内存里的，**内存管理单元** （*MMU*）就做将虚拟内存地址转换成物理地址的工作。

而当进程访问的虚拟地址在页表中查不到时，系统会产生一个**缺页异常**，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。

> 分页是怎么解决分段的「外部内存碎片和内存交换效率低」的问题？

内存分页由于内存空间都是预先划分好的，也就不会像内存分段一样，在段与段之间会产生间隙非常小的内存，这正是分段会产生外部内存碎片的原因。而**采用了分页，页与页之间是紧密排列的，所以不会有外部碎片。**

但是，因为内存分页机制分配内存的最小单位是一页，即使程序不足一页大小，我们最少只能分配一个页，所以页内会出现内存浪费，所以针对**内存分页机制会有内部内存碎片**的现象。

如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为**换出**（*Swap Out*）。一旦需要的时候，再加载进来，称为**换入**（*Swap In*）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，**内存交换的效率就相对比较高。**

![img](https://cdn.xiaolincoding.com//mysql/other/388a29f45fe947e5a49240e4eff13538-20230309234651917.png)

更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。我们完全可以在进行虚拟内存和物理内存的页之间的映射之后，并不真的把页加载到物理内存里，而是**只有在程序运行中，需要用到对应虚拟内存页里面的指令和数据时，再加载到物理内存里面去。**

> 分页机制下，虚拟地址和物理地址是如何映射的？

在分页机制下，虚拟地址分为两部分，**页号**和**页内偏移**。页号作为页表的索引，**页表**包含物理页每页所在**物理内存的基地址**，这个基地址与页内偏移的组合就形成了物理内存地址，见下图。

![img](https://cdn.xiaolincoding.com//mysql/other/7884f4d8db4949f7a5bb4bbd0f452609.png)

总结一下，对于一个内存地址转换，其实就是这样三个步骤：

- 把虚拟内存地址，切分成页号和偏移量；
- 根据页号，从页表里面，查询对应的物理页号；
- 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

下面举个例子，虚拟内存中的页通过页表映射为了物理内存中的页，如下图：

![img](https://cdn.xiaolincoding.com//mysql/other/8f187878c809414ca2486b0b71e8880e.png)

这看起来似乎没什么毛病，但是放到实际中操作系统，这种简单的分页是肯定是会有问题的。

> 简单的分页有什么缺陷吗？

有空间上的缺陷。

因为操作系统是可以同时运行非常多的进程的，那这不就意味着页表会非常的庞大。

在 32 位的环境下，虚拟地址空间共有 4GB，假设一个页的大小是 4KB（2^12），那么就需要大约 100 万 （2^20） 个页，每个「页表项」需要 4 个字节大小来存储，那么整个 4GB 空间的映射就需要有 `4MB` 的内存来存储页表。

这 4MB 大小的页表，看起来也不是很大。但是要知道每个进程都是有自己的虚拟地址空间的，也就说都有自己的页表。

那么，`100` 个进程的话，就需要 `400MB` 的内存来存储页表，这是非常大的内存了，更别说 64 位的环境了。

  所以后续提出多级页表。。。。。。。。。。

## 内存满了会发生什么

先来说说第一个问题：虚拟内存有什么作用？

- 第一，虚拟内存可以使得进程对运行内存超过物理内存大小，因为程序运行符合局部性原理，CPU 访问内存会有很明显的重复访问的倾向性，对于那些没有被经常使用到的内存，我们可以把它换出到物理内存之外，比如硬盘上的 swap 区域。
- 第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。
- 第三，页表里的页表项中除了物理地址之外，还有一些标记属性的比特，比如控制一个页的读写权限，标记该页是否存在等。在内存访问方面，操作系统提供了更好的安全性。

> 第二，由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题。可是如果不知道其他进程怎么保证内存页交换的时候更新其他进程页表的映射
>
> 
>
> ChatGPT
>
> ChatGPT
>
> 在一个操作系统中，虚拟内存管理的实现确实需要考虑到多个进程之间的页表更新和内存页交换。当一个进程需要与其他进程共享物理内存或者进行内存页的交换时，操作系统会采取一些机制来确保页表的正确更新和维护。以下是一些常见的方法：
>
> 1. **内核控制**：操作系统内核负责管理和调度进程之间的内存交换。当需要进行内存页的交换时，内核会负责协调各个进程的页表更新。内核可以通过锁定页表、暂时中断进程等方式来确保页表的一致性和正确性。
> 2. **通知机制**：操作系统可以通过通知机制告知其他进程发生了内存页的交换或者更新。其他进程收到通知后，可以相应地更新自己的页表信息。
> 3. **共享内存**：在一些情况下，多个进程可能需要共享同一块物理内存区域。在这种情况下，操作系统会为这些共享内存区域维护一个共享的页表，多个进程共享同一个页表项。
> 4. **页表刷新**：操作系统定期检查页表的状态，并确保它们与实际的物理内存保持一致。当发生内存页交换或者其他进程的页表更新时，操作系统会刷新相关的页表项，以确保所有进程的页表都是最新的。
>
> 综上所述，操作系统通过内核控制、通知机制、共享内存和页表刷新等方式来保证多个进程之间的页表更新和内存页交换的正确性和一致性。
>
> 
>
> You
>
> 那由于每个进程都有自己的页表，所以每个进程的虚拟内存空间就是相互独立的。进程也没有办法访问其他进程的页表，所以这些页表是私有的，这就解决了多进程之间地址冲突的问题  是什么意思
>
> 
>
> ChatGPT
>
> ChatGPT
>
> 这段话描述了操作系统中多进程环境下的虚拟内存管理机制。
>
> 1. **每个进程有自己的页表**：页表是操作系统用来管理虚拟内存和物理内存之间映射关系的数据结构。由于每个进程都有自己的页表，因此每个进程的虚拟内存空间是独立的。这意味着每个进程都可以拥有自己的地址空间，不受其他进程影响。
> 2. **进程无法访问其他进程的页表**：由于操作系统设计，一个进程无法直接访问其他进程的页表。这意味着进程之间无法直接查看或修改彼此的虚拟内存映射关系，从而确保了各个进程之间的隔离性和安全性。
> 3. **解决了多进程之间地址冲突的问题**：由于每个进程都有自己的独立虚拟内存空间和页表，不同进程之间的地址空间是相互隔离的，因此不会发生地址冲突问题。即使多个进程使用相同的虚拟地址，由于它们对应的物理地址会在各自的页表中进行映射，因此不会产生冲突。
>
> 总之，这段话强调了操作系统通过为每个进程分配独立的页表和虚拟内存空间来确保多进程环境下的内存隔离和安全性。



## 内存分配的过程是怎样的？

应用程序==通过 malloc函数==申请内存的时候，实际上申请的是==虚拟内存==，此时并不会分配物理内存。

当应用程序读写了这块虚拟内存，CPU 就会去访问这个虚拟内存， 这时会发现这个虚拟内存没有映射到物理内存， CPU 就会产生**缺页中断**，进程会从用户态切换到内核态，并将缺页中断交给内核的 Page Fault Handler （缺页中断函数）处理。

缺页中断处理函数会看是否有空闲的物理内存，如果有，就直接分配物理内存，并建立虚拟内存与物理内存之间的映射关系。

如果没有空闲的物理内存，那么内核就会开始进行**回收内存**的工作，回收的方式主要是两种：直接内存回收和后台内存回收。

- **后台内存回收**（kswapd）：在物理内存紧张的时候，会唤醒 kswapd 内核线程来回收内存，这个回收内存的过程**异步**的，不会阻塞进程的执行。
- **直接内存回收**（direct reclaim）：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是**同步**的，会阻塞进程的执行。

如果直接内存回收后，空闲的物理内存仍然无法满足此次物理内存的申请，那么内核就会放最后的大招了 ——**触发 OOM （Out of Memory）机制**。

OOM Killer 机制会根据算法选择一个占用物理内存较高的进程，然后将其杀死，以便释放内存资源，如果物理内存依然不足，OOM Killer 会继续杀死占用物理内存较高的进程，直到释放足够的内存位置。

申请物理内存的过程如下图：

![img](https://cdn.xiaolincoding.com//mysql/other/2f61b0822b3c4a359f99770231981b07.png)

### [#](https://www.xiaolincoding.com/os/3_memory/mem_reclaim.html#哪些内存可以被回收)哪些内存可以被回收？

系统内存紧张的时候，就会进行回收内存的工作，那具体哪些内存是可以被回收的呢？

主要有两类内存可以被回收，而且它们的回收方式也不同。

- **文件页**（File-backed Page）：内核缓存的磁盘数据（Buffer）和内核缓存的文件数据（Cache）都叫作文件页。大部分文件页，都可以直接释放内存，以后有需要时，再从磁盘重新读取就可以了。而那些被应用程序修改过，并且暂时还没写入磁盘的数据（也就是脏页），就得先写入磁盘，然后才能进行内存释放。所以，**回收干净页的方式是直接释放内存，回收脏页的方式是先写回磁盘后再释放内存**。
- **匿名页**（Anonymous Page）：这部分内存没有实际载体，不像文件缓存有硬盘文件这样一个载体，比如堆、栈数据等。这部分内存很可能还要再次被访问，所以不能直接释放内存，它们**回收的方式是通过 Linux 的 Swap 机制**，Swap 会把不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。

文件页和匿名页的回收都是基于 LRU 算法，也就是优先回收不常访问的内存。LRU 回收算法，实际上维护着 active 和 inactive 两个双向链表，其中：

- **active_list** 活跃内存页链表，这里存放的是最近被访问过（活跃）的内存页；
- **inactive_list** 不活跃内存页链表，这里存放的是很少被访问（非活跃）的内存页；

越接近链表尾部，就表示内存页越不常访问。这样，在回收内存时，系统就可以根据活跃程度，优先回收不活跃的内存。

活跃和非活跃的内存页，按照类型的不同，又分别分为文件页和匿名页。可以从 /proc/meminfo 中，查询它们的大小，比如：

```shell
# grep表示只保留包含active的指标（忽略大小写）
# sort表示按照字母顺序排序
[root@xiaolin ~]# cat /proc/meminfo | grep -i active | sort
Active:           901456 kB
Active(anon):     227252 kB
Active(file):     674204 kB
Inactive:         226232 kB
Inactive(anon):    41948 kB
Inactive(file):   184284 kB
```

### [#](https://www.xiaolincoding.com/os/3_memory/mem_reclaim.html#回收内存带来的性能影响)回收内存带来的性能影响

在前面我们知道了回收内存有两种方式。

- 一种是后台内存回收，也就是唤醒 kswapd 内核线程，这种方式是异步回收的，不会阻塞进程。
- 一种是直接内存回收，这种方式是同步回收的，会阻塞进程，这样就会造成很长时间的延迟，以及系统的 CPU 利用率会升高，最终引起系统负荷飙高。

可被回收的内存类型有文件页和匿名页：

- 文件页的回收：对于干净页是直接释放内存，这个操作不会影响性能，而对于脏页会先写回到磁盘再释放内存，这个操作会发生磁盘 I/O 的，这个操作是会影响系统性能的。
- 匿名页的回收：如果开启了 Swap 机制，那么 Swap 机制会将不常访问的匿名页换出到磁盘中，下次访问时，再从磁盘换入到内存中，这个操作是会影响系统性能的。

可以看到，回收内存的操作基本都会发生磁盘 I/O 的，如果回收内存的操作很频繁，意味着磁盘 I/O 次数会很多，这个过程势必会影响系统的性能，整个系统给人的感觉就是很卡。

下面针对回收内存导致的性能影响，说说常见的解决方式。

### [#](https://www.xiaolincoding.com/os/3_memory/mem_reclaim.html#调整文件页和匿名页的回收倾向)调整文件页和匿名页的回收倾向

从文件页和匿名页的回收操作来看，文件页的回收操作对系统的影响相比匿名页的回收操作会少一点，因为文件页对于干净页回收是不会发生磁盘 I/O 的，而匿名页的 Swap 换入换出这两个操作都会发生磁盘 I/O。

Linux 提供了一个 `/proc/sys/vm/swappiness` 选项，用来调整文件页和匿名页的回收倾向。

swappiness 的范围是 0-100，数值越大，越积极使用 Swap，也就是更倾向于回收匿名页；数值越小，越消极使用 Swap，也就是更倾向于回收文件页。

```shell
[root@xiaolin ~]# cat /proc/sys/vm/swappiness
0
```

一般建议 swappiness 设置为 0（默认值是 60），这样在回收内存的时候，会更倾向于文件页的回收，但是并不代表不会回收匿名页。

### [#](https://www.xiaolincoding.com/os/3_memory/mem_reclaim.html#尽早触发-kswapd-内核线程异步回收内存)尽早触发 kswapd 内核线程异步回收内存

> 如何查看系统的直接内存回收和后台内存回收的指标？

我们可以使用 `sar -B 1` 命令来观察：

![img](https://cdn.xiaolincoding.com//mysql/other/8acb6b28d0fc4858bd57be147d087def.png)

图中红色框住的就是后台内存回收和直接内存回收的指标，它们分别表示：

- pgscank/s : kswapd(后台回收线程) 每秒扫描的 page 个数。
- pgscand/s: 应用程序在内存申请过程中每秒直接扫描的 page 个数。
- pgsteal/s: 扫描的 page 中每秒被回收的个数（pgscank+pgscand）。

如果系统时不时发生抖动，并且在抖动的时间段里如果通过 sar -B 观察到 pgscand 数值很大，那大概率是因为「直接内存回收」导致的。

针对这个问题，解决的办法就是，可以通过尽早的触发「后台内存回收」来避免应用程序进行直接内存回收。

> 什么条件下才能触发 kswapd 内核线程回收内存呢？

内核定义了三个内存阈值（watermark，也称为水位），用来衡量当前剩余内存（pages_free）是否充裕或者紧张，分别是：

- 页最小阈值（pages_min）；
- 页低阈值（pages_low）；
- 页高阈值（pages_high）；

这三个内存阈值会划分为四种内存使用情况，如下图：

![img](https://cdn.xiaolincoding.com//mysql/other/166bc9f5b7c545d89f1e36ab8dd772cf.png)

kswapd 会定期扫描内存的使用情况，根据剩余内存（pages_free）的情况来进行内存回收的工作。

- 图中绿色部分：如果剩余内存（pages_free）大于 页高阈值（pages_high），说明剩余内存是充足的；
- 图中蓝色部分：如果剩余内存（pages_free）在页高阈值（pages_high）和页低阈值（pages_low）之间，说明内存有一定压力，但还可以满足应用程序申请内存的请求；
- 图中橙色部分：如果剩余内存（pages_free）在页低阈值（pages_low）和页最小阈值（pages_min）之间，说明内存压力比较大，剩余内存不多了。**这时 kswapd0 会执行内存回收，直到剩余内存大于高阈值（pages_high）为止**。虽然会触发内存回收，但是不会阻塞应用程序，因为两者关系是异步的。
- 图中红色部分：如果剩余内存（pages_free）小于页最小阈值（pages_min），说明用户可用内存都耗尽了，此时就会**触发直接内存回收**，这时应用程序就会被阻塞，因为两者关系是同步的。



## 进程管理

在 Linux 操作系统中，虚拟地址空间的内部又被分为内核空间和用户空间两部分，不同位数的系统，地址 空间的范围也不同。比如最常⻅的 32 位和 64 位系统，如下所示:

![img](https://cdn.xiaolincoding.com//mysql/other/20210715092026648.png)

通过这里可以看出:

- 32 位系统的内核空间占用 1G ，位于最高处，剩下的 3G 是用户空间;
- 64 位系统的内核空间和用户空间都是 128T ，分别占据整个内存空间的最高和最低处，剩下的中 间部分是未定义的。







## malloc和mmap

> **mmap**区别**内存管理单元（MMU）**：
>
> - MMU负责将虚拟地址转换为物理地址。它使用当前进程的页表信息来进行地址转换，因此每个进程的地址空间是独立的，不会互相干扰
>

> 1. **高效文件I/O**：
>    - 通过内存映射，文件内容可以直接在内存中操作，避免了多次系统调用带来的开销。
> 2. **进程间通信**：
>    - 共享内存映射允许多个进程共享一块内存区域，实现高效的进程间通信。
> 3. **设备内存访问**：
>    - 通过映射设备内存，进程可以直接访问硬件设备的寄存器和内存。



## 进程间通信？

> mmap怎么实现多进程的共享内存的？mmap映射的应该是虚拟内存地址吧，不同进程经过虚拟地址转换后就不一样了吧
>
> 
>
> `mmap` 的确是映射虚拟内存地址的，但通过某些特定的配置，可以实现多进程共享同一块物理内存，从而实现进程间的共享内存。具体实现方式如下：
>
> ### 使用 `mmap` 实现多进程共享内存
>
> 1. **使用共享内存文件**：
>    - 可以使用一个文件或通过特殊的共享内存文件（如`/dev/shm`）来实现共享。
> 2. **使用 `MAP_SHARED` 标志**：
>    - 在调用 `mmap` 时，使用 `MAP_SHARED` 标志，表示映射的内存区域是共享的，所有对该区域的修改对所有映射该区域的进程都是可见的。对该区域的修改对所有映射该区域的进程都是可见的。

# 文件系统

### 文件的存储

> 当你在Linux系统中挂载一个移动硬盘时，文件系统会读取一些特定的信息并将其加载到内存中，以便系统能够正常访问和管理该文件系统。这些信息主要包括以下几个部分：
>
> ### 1. 超级块（Superblock）
>
> 超级块是文件系统的一个关键数据结构，包含了整个文件系统的重要信息。每个文件系统都有一个超级块，当挂载文件系统时，超级块会被读取到内存中。超级块包含的信息包括：
>
> - 文件系统类型
> - 文件系统大小
> - 块大小
> - 空闲块和inode计数
> - 文件系统状态
> - 其他文件系统特定的信息
>
> ### 2. 组描述符（Group Descriptors）
>
> 对于如ext3、ext4等文件系统，超级块后面跟着的是一组组描述符，每个组描述符描述了文件系统中一个块组的元数据信息。组描述符会被读取到内存中，用于管理文件系统的分配策略和状态。
>
> ### 3. 块位图和inode位图（Block Bitmap and Inode Bitmap）
>
> 这些位图用于跟踪文件系统中块和inode的分配状态。位图会被读取到内存中，以便在文件创建、删除或修改时快速更新。
>
> ### 4. inode表（Inode Table）
>
> inode表包含了文件系统中所有文件和目录的元数据信息（如文件大小、权限、时间戳、数据块指针等）。在挂载时，只有必要的一部分inode会被读取到内存中，而其他inode会在访问时按需加载。
>
> ### 5. 目录项（Directory Entries）
>
> 目录项包含了目录中所有文件和子目录的列表。在挂载时，根目录的目录项通常会被读取到内存中，以便能够快速访问文件系统的根目录。
>
> ### 6. 日志（Journal）
>
> 对于带有日志功能的文件系统（如ext3、ext4、btrfs等），文件系统在挂载时会读取日志以恢复未完成的事务，从而确保文件系统的一致性和完整性。
>
> ### 7. 文件系统缓存（Page Cache, Dentry Cache, Inode Cache）
>
> 为了提高文件系统的性能，Linux会将文件系统的元数据和文件数据缓存到内存中。这些缓存包括：
>
> - **Page Cache**：用于缓存文件数据的页面。
> - **Dentry Cache**：用于缓存目录项。
> - **Inode Cache**：用于缓存inode信息。
>
> ### 挂载过程
>
> 挂载文件系统时的具体过程如下：
>
> 1. **读取超级块**：操作系统读取文件系统的超级块，并验证文件系统类型和完整性。
> 2. **初始化数据结构**：操作系统初始化与文件系统相关的数据结构，如组描述符、位图、inode表等。
> 3. **恢复日志**：如果文件系统具有日志功能，操作系统会读取并回放日志，以确保文件系统的一致性。
> 4. **挂载根目录**：操作系统将文件系统的根目录挂载到指定的挂载点，并将相关的目录项和inode信息加载到内存中。
> 5. **准备缓存**：操作系统准备文件系统的缓存，以提高后续文件访问的性能
>
> 
>
> 
>
> 4o
>
> 这些信息是存在磁盘的还是插上硬盘后构建的数据结构
>
> 
>
> ChatGPT
>
> 这些信息（超级块、组描述符、块位图、inode位图、inode表、目录项、日志等）主要存储在磁盘上。当你插入并挂载硬盘后，操作系统会从磁盘读取这些信息并构建相应的数据结构在内存中，以便于后续的文件操作



### 空闲管理是位图法

### MMap

**文件映射区**：通过`mmap`将文件或设备内容映射到进程的虚拟地址空间，使得应用程序可以通过内存地址直接访问文件内容。

**多进程共享内存**：使用`mmap`和`MAP_SHARED`可以实现多个进程间共享内存，允许它们通过映射区共享数据。

### 大文件传输+ MMap + 零拷贝

==传统方法==

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png)

期间共**发生了 4 次用户态与内核态的上下文切换**，因为发生了两次系统调用，一次是 `read()` ，一次是 `write()`，

==mmap==

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20%2B%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png)

具体过程如下：

- 应用进程调用了 `mmap()` 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里。接着，应用进程跟操作系统内核「共享」这个缓冲区；
- 应用进程再调用 `write()`，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；
- 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。



==sendfile==

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png)

但是这还不是真正的零拷贝技术，如果网卡支持 SG-DMA（*The Scatter-Gather Direct Memory Access*）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。



### Page Cache

最后一个缺陷是在某些应用场景下比 Direct I/O 多一次磁盘读 I/O 以及磁盘写 I/O

可能是触发swap的情况下吧







### 命令

ifconfig



ip



netstat



ss

当 socket 状态处于 `Established`时：

- *Recv-Q* 表示 socket 缓冲区中还没有被应用程序读取的字节数；
- *Send-Q* 表示 socket 缓冲区中还没有被远端主机确认的字节数；

而当 socket 状态处于 `Listen` 时：

- *Recv-Q* 表示全连接队列的长度；
- *Send-Q* 表示全连接队列的最大长度；

















